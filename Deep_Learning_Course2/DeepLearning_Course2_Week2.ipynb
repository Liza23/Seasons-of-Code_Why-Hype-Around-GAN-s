{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning.Course2.Week2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOg2YBAK0JAixvGNs/8K4+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liza23/Seasons-of-Code_Why-Hype-Around-GAN-s/blob/master/DeepLearning_Course2_Week2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxA_We1Jt7VN",
        "colab_type": "text"
      },
      "source": [
        "Optimization Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80BNNfbRuDFF",
        "colab_type": "text"
      },
      "source": [
        "Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SxnmGU5tQVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import math\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "\n",
        "from opt_utils_v1a import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
        "from opt_utils_v1a import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
        "from testCases import *\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pmRPGabuNJ1",
        "colab_type": "text"
      },
      "source": [
        "Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3Xoy7F5uO5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: update_parameters_with_gd\n",
        "\n",
        "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using one step of gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters to be updated:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients to update each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    for l in range(L):\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9Z7m1oCCQ7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters, grads, learning_rate = update_parameters_with_gd_test_case()\n",
        "\n",
        "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
        "print(\"W1 =\\n\" + str(parameters[\"W1\"]))\n",
        "print(\"b1 =\\n\" + str(parameters[\"b1\"]))\n",
        "print(\"W2 =\\n\" + str(parameters[\"W2\"]))\n",
        "print(\"b2 =\\n\" + str(parameters[\"b2\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2IZEm7ZCZAn",
        "colab_type": "text"
      },
      "source": [
        "Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yzskkviCcGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: random_mini_batches\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
        "    mini_batch_size -- size of the mini-batches, integer\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "        \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
        "        ### END CODE HERE ###\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        mini_batch_X = shuffled_X[:,mini_batch_size*(num_complete_minibatches):m]\n",
        "        mini_batch_Y = shuffled_Y[:,mini_batch_size*(num_complete_minibatches):m]\n",
        "        ### END CODE HERE ###\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hppgsXt_CedS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()\n",
        "mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)\n",
        "\n",
        "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
        "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
        "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
        "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
        "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
        "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
        "print (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg1MnaHaCkut",
        "colab_type": "text"
      },
      "source": [
        "Momentum Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_kJXs62Cllv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: initialize_velocity\n",
        "\n",
        "def initialize_velocity(parameters):\n",
        "    \"\"\"\n",
        "    Initializes the velocity as a python dictionary with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    \n",
        "    Returns:\n",
        "    v -- python dictionary containing the current velocity.\n",
        "                    v['dW' + str(l)] = velocity of dWl\n",
        "                    v['db' + str(l)] = velocity of dbl\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    \n",
        "    # Initialize velocity\n",
        "    for l in range(L):\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\"+str(l+1)].shape[0],parameters[\"W\"+str(l+1)].shape[1]))\n",
        "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\"+str(l+1)].shape[0],parameters[\"b\"+str(l+1)].shape[1]))\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhI43NaBDaoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = initialize_velocity_test_case()\n",
        "\n",
        "v = initialize_velocity(parameters)\n",
        "print(\"v[\\\"dW1\\\"] =\\n\" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] =\\n\" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] =\\n\" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] =\\n\" + str(v[\"db2\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiARNNU2DjOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: update_parameters_with_momentum\n",
        "\n",
        "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using Momentum\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- python dictionary containing the current velocity:\n",
        "                    v['dW' + str(l)] = ...\n",
        "                    v['db' + str(l)] = ...\n",
        "    beta -- the momentum hyperparameter, scalar\n",
        "    learning_rate -- the learning rate, scalar\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    v -- python dictionary containing your updated velocities\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    \n",
        "    # Momentum update for each parameter\n",
        "    for l in range(L):\n",
        "        \n",
        "        ### START CODE HERE ### (approx. 4 lines)\n",
        "        # compute velocities\n",
        "        v[\"dW\" + str(l+1)] = beta*v[\"dW\"+str(l+1)]+(1-beta)*grads[\"dW\"+str(l+1)]\n",
        "        v[\"db\" + str(l+1)] = beta*v[\"db\"+str(l+1)]+(1-beta)*grads[\"db\"+str(l+1)]\n",
        "        # update parameters\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)]-learning_rate*v[\"dW\"+str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)]-learning_rate*v[\"db\"+str(l+1)]\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return parameters, v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV2JjTUlDoE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters, grads, v = update_parameters_with_momentum_test_case()\n",
        "\n",
        "parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\n",
        "print(\"W1 = \\n\" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \\n\" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \\n\" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \\n\" + str(parameters[\"b2\"]))\n",
        "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] = v\" + str(v[\"db2\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNq06BR9Dubr",
        "colab_type": "text"
      },
      "source": [
        "Adam Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIwanbAoDwyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: initialize_adam\n",
        "\n",
        "def initialize_adam(parameters) :\n",
        "    \"\"\"\n",
        "    Initializes v and s as two python dictionaries with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters[\"W\" + str(l)] = Wl\n",
        "                    parameters[\"b\" + str(l)] = bl\n",
        "    \n",
        "    Returns: \n",
        "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
        "                    v[\"dW\" + str(l)] = ...\n",
        "                    v[\"db\" + str(l)] = ...\n",
        "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
        "                    s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "    ### START CODE HERE ### (approx. 4 lines)\n",
        "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\"+str(l+1)].shape[0],parameters[\"W\"+str(l+1)].shape[1]))\n",
        "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\"+str(l+1)].shape[0],parameters[\"b\"+str(l+1)].shape[1]))\n",
        "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\"+str(l+1)].shape[0],parameters[\"W\"+str(l+1)].shape[1]))\n",
        "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\"+str(l+1)].shape[0],parameters[\"b\"+str(l+1)].shape[1]))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zMOTfP-D0MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = initialize_adam_test_case()\n",
        "\n",
        "v, s = initialize_adam(parameters)\n",
        "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\n",
        "print(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\n",
        "print(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\n",
        "print(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\n",
        "print(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpIc5Zy4D7Zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: update_parameters_with_adam\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\"+str(l+1)]+(1-beta1)*grads[\"dW\"+str(l+1)]\n",
        "        v[\"db\" + str(l+1)] = beta1*v[\"db\"+str(l+1)]+(1-beta1)*grads[\"db\"+str(l+1)]\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-beta1**t)\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-beta1**t)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\"+str(l+1)]+(1-beta2)*np.square(grads[\"dW\"+str(l+1)])\n",
        "        s[\"db\" + str(l+1)] = beta2*s[\"db\"+str(l+1)]+(1-beta2)*np.square(grads[\"db\"+str(l+1)])\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-beta2**t)\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-beta2**t)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*(v_corrected[\"dW\" + str(l+1)]/(np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon))\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*(v_corrected[\"db\" + str(l+1)]/(np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon))\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return parameters, v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FKPHP5OEOJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters, grads, v, s = update_parameters_with_adam_test_case()\n",
        "parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)\n",
        "\n",
        "print(\"W1 = \\n\" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \\n\" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \\n\" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \\n\" + str(parameters[\"b2\"]))\n",
        "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\n",
        "print(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\n",
        "print(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\n",
        "print(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\n",
        "print(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wn99y2pEU7e",
        "colab_type": "text"
      },
      "source": [
        "Model with different optimisation algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhXTzMSXEcvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X, train_Y = load_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a81cC141EfyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
        "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
        "    \"\"\"\n",
        "    3-layer neural network model which can be run in different optimizer modes.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (2, number of examples)\n",
        "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
        "    layers_dims -- python list, containing the size of each layer\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    mini_batch_size -- the size of a mini batch\n",
        "    beta -- Momentum hyperparameter\n",
        "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
        "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "    num_epochs -- number of epochs\n",
        "    print_cost -- True to print the cost every 1000 epochs\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "\n",
        "    L = len(layers_dims)             # number of layers in the neural networks\n",
        "    costs = []                       # to keep track of the cost\n",
        "    t = 0                            # initializing the counter required for Adam update\n",
        "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
        "    m = X.shape[1]                   # number of training examples\n",
        "    \n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "    # Initialize the optimizer\n",
        "    if optimizer == \"gd\":\n",
        "        pass # no initialization required for gradient descent\n",
        "    elif optimizer == \"momentum\":\n",
        "        v = initialize_velocity(parameters)\n",
        "    elif optimizer == \"adam\":\n",
        "        v, s = initialize_adam(parameters)\n",
        "    \n",
        "    # Optimization loop\n",
        "    for i in range(num_epochs):\n",
        "        \n",
        "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
        "        seed = seed + 1\n",
        "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
        "        cost_total = 0\n",
        "        \n",
        "        for minibatch in minibatches:\n",
        "\n",
        "            # Select a minibatch\n",
        "            (minibatch_X, minibatch_Y) = minibatch\n",
        "\n",
        "            # Forward propagation\n",
        "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
        "\n",
        "            # Compute cost and add to the cost total\n",
        "            cost_total += compute_cost(a3, minibatch_Y)\n",
        "\n",
        "            # Backward propagation\n",
        "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
        "\n",
        "            # Update parameters\n",
        "            if optimizer == \"gd\":\n",
        "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
        "            elif optimizer == \"momentum\":\n",
        "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
        "            elif optimizer == \"adam\":\n",
        "                t = t + 1 # Adam counter\n",
        "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
        "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
        "        cost_avg = cost_total / m\n",
        "        \n",
        "        # Print the cost every 1000 epoch\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost_avg)\n",
        "                \n",
        "    # plot the cost\n",
        "    plt.plot(costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('epochs (per 100)')\n",
        "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pGS-P-ZElRr",
        "colab_type": "text"
      },
      "source": [
        "* Mini-batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHn_b7TKEiKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train 3-layer model\n",
        "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
        "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n",
        "\n",
        "# Predict\n",
        "predictions = predict(train_X, train_Y, parameters)\n",
        "\n",
        "# Plot decision boundary\n",
        "plt.title(\"Model with Gradient Descent optimization\")\n",
        "axes = plt.gca()\n",
        "axes.set_xlim([-1.5,2.5])\n",
        "axes.set_ylim([-1,1.5])\n",
        "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nlqcjtPEwwK",
        "colab_type": "text"
      },
      "source": [
        "* Mini-batch Gradient Descent with Momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhyT8rWSEtEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train 3-layer model\n",
        "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
        "parameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = \"momentum\")\n",
        "\n",
        "# Predict\n",
        "predictions = predict(train_X, train_Y, parameters)\n",
        "\n",
        "# Plot decision boundary\n",
        "plt.title(\"Model with Momentum optimization\")\n",
        "axes = plt.gca()\n",
        "axes.set_xlim([-1.5,2.5])\n",
        "axes.set_ylim([-1,1.5])\n",
        "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Ngjy_cE_u5",
        "colab_type": "text"
      },
      "source": [
        "* Mini-batch with Adam Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czFfljxBFEOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train 3-layer model\n",
        "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
        "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\")\n",
        "\n",
        "# Predict\n",
        "predictions = predict(train_X, train_Y, parameters)\n",
        "\n",
        "# Plot decision boundary\n",
        "plt.title(\"Model with Adam optimization\")\n",
        "axes = plt.gca()\n",
        "axes.set_xlim([-1.5,2.5])\n",
        "axes.set_ylim([-1,1.5])\n",
        "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}